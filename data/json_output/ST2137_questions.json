[
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "1",
    "question_type": "coding",
    "difficulty_rating_manual": 0.41,
    "difficulty_rating_model": null,
    "question_stem": "This question is to be answered using Python. Consider the stud_perf dataset once again. There were three measures of performance of the students:\nFeature Description Details\nG1 First period grade from 0 to 20\nG2 Second period grade from 0 to 20\nG3 Final grade from 0 to 20\nIn this question, we shall investigate the agreement between G1 and G3 scores. If there is a high agreement, it means that teachers can drop one of the scores and still get a good estimate of final grade of a student.\n1. First, read in the dataset, and divide the G1 and G3 scores into 10 bins:\n* [0,2], (2,4], (4,6], (6,8], (8,10], (10,12], (12,14], (14,16], (16,18], (18,20]\n* You should now have two new columns G1_bin and G3_bin in the dataset.\n2. Next, create a contingency table with G1_bin in the rows and G3_bin in the columns.\n3. Convert the cell counts into proportions. The sum of entries in all cells should now equal to 1. Create a visualisation of this table of proportions that is relevant to the goal of measuring agreement.\n4. If we let pij be the proportion in row i and column j, then the strictest version of agreement is η0=∑i=110pii What is the range of values for η0? Which values correspond to higher agreement?\n5. A less stringent measure of agreement is given by η1=∑|i−j|≤1pij Compute η1 separately for the five groups defined by Medu and summarise what you observe.",
    "question_stem_html": null,
    "concept_tags": [
      "data binning",
      "contingency table",
      "agreement measures"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "2",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.23,
    "difficulty_rating_model": null,
    "question_stem": "In 1935, Sir R A Fisher described an experiment involving a British woman. The woman claimed that if she was presented with a cup of milk tea, she would be able to distinguish whether milk or tea was added to the cup first. To test, she was given 8 cups of tea, in four of which milk was added first.\nThe data collected was as follows:\nActual Milk Actual Tea\nGuessed Milk 3 1\nGuessed Tea 1 3\nSuppose that Fisher’s Exact Test was applied to assess if there was any association between her guesses and the truth. Under the null hypothesis of Fisher’s Exact Test, what is the probability of observing the table above?",
    "question_stem_html": null,
    "concept_tags": [
      "Fisher's exact test",
      "hypothesis testing",
      "contingency table"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "3",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.97,
    "difficulty_rating_model": null,
    "question_stem": "Consider the following two histograms (created with Python) from the liverpool dataset used in Tutorial 2:\nWhat is the argument needed to convert the Histogram A into Histogram B?\nliverpool.GF.hist(<argument>)",
    "question_stem_html": null,
    "concept_tags": [
      "data visualization",
      "histograms",
      "Python pandas"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "4",
    "question_type": "coding",
    "difficulty_rating_manual": 0.44,
    "difficulty_rating_model": null,
    "question_stem": "The data in phones.csv contains information on phone calls made in Belgium from 1950 until 1973. Let Y be the number of calls, and X be the year variable. Read the data into Python as a pandas dataframe and answer the following questions:\n1. Write a function that takes in three arguments: beta0, beta1, and the phones dataframe. It should compute the following L1-norm and return it: ∑i=1n|Yi−β0−β1Xi|\n2. Iterate over a range of beta0 and beta1 values and find the pair that minimizes the L1-norm. Return this pair of beta0 and beta1 values.\n3. Create a plot of this line, along with the OLS estimate, along with the datapoints.\n4. Discuss the benefits of the L1-fit over the OLS fit.",
    "question_stem_html": null,
    "concept_tags": [
      "L1 regression",
      "OLS regression",
      "Python programming"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "5",
    "question_type": "coding",
    "difficulty_rating_manual": 0.71,
    "difficulty_rating_model": null,
    "question_stem": "Suppose that the data in liverpool_2223_season.csv has been read into Python as liverpool. The following code tabulates the goal counts for and stores them in a column in goal_counts.\ngoal_counts =pd.DataFrame(np.zeros((10, 2), dtype='int'),\ncolumns=['GF', 'GA'])\ntmp2 =liverpool.GF.value_counts()\ngoal_counts.loc[tmp2.index, 'GF'] =tmp2\nContinue the code to fill up the second column, which tabulates the goal counts against into the second column of goal_counts. Then create the following bar chart, which compares the GF and GA:",
    "question_stem_html": null,
    "concept_tags": [
      "pandas dataframe",
      "data manipulation",
      "bar chart"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "6",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.99,
    "difficulty_rating_model": null,
    "question_stem": "What is the most likely solution to the error below?",
    "question_stem_html": null,
    "concept_tags": [
      "Python errors",
      "pandas import",
      "data reading"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "7",
    "question_type": "coding",
    "difficulty_rating_manual": 0.42,
    "difficulty_rating_model": null,
    "question_stem": "The heifers dataset was introduced on in the topic on ANOVA. An analyst used SAS to run the ANOVA procedure and estimate the contrast comparing the Control group to the rest of the five groups. This is the code that the analyst used:\nproc glm data=ST2137.HEIFERS;\nclass type;\nmodel org=type / clparm;\nmeans type / hovtest=levene welch plots=none;\nlsmeans type / adjust=tukey pdiff alpha=.05;\nestimate 'control vs. rest' type -1 5 -1 -1 -1 -1 / divisor=5;\nrun;\nThe essential output for this particular contrast can be seen in the following figures:\nUse Python to recreate\n1. the point estimate of the contrast, and\n2. the confidence interval for the contrast.",
    "question_stem_html": null,
    "concept_tags": [
      "ANOVA",
      "contrasts",
      "confidence interval"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "8",
    "question_type": "coding",
    "difficulty_rating_manual": 0.52,
    "difficulty_rating_model": null,
    "question_stem": "This question is to be answered using Python. In Tutorial 6, we considered the notion of a sensitivity curve for assessing the robustness of an estimator. Suppose we have an estimator based on n observations: Tn(x1,x2,…,xn) The sensitivity curve sc(x) when a new observation x is added is given by sc(x)=(n+1)×[Tn+1(x1,…,xn,x)−Tn(x1,…,xn)] Let us consider a newly proposed robust estimator called the broadened median. Given a sample of size n, here is how it is computed:\n* For n odd, the broadened median is\n* the average of the three central order statistics for 5≤n≤12.\n* the average of the five central order statistics for n≥13.\n* For n even, the broadened median is\n* the weighted average of the central four order statistics for 5≤n≤12, with weights 1/6,1/3,1/3 and 1/6.\n* the weighted average of the central six order statistics for n≥13, with weights 1/10,1/5,1/5,1/5,1/5 and 1/10.\nSuppose we have the following sample of 10 points (n=10):\n2,4,6,7,8,10,14,19,21,28\n1. Plot the sensitivity curve for x∈[5,25] when T represents the broadened median.\n2. Which of the robust estimators that we covered in class is most similar to the broadened median? Explain your answer.\nNote that your code does not have to handle the general case of estimator. It only has to work for the above dataset.",
    "question_stem_html": null,
    "concept_tags": [
      "robust estimators",
      "sensitivity curve",
      "median"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "9",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.99,
    "difficulty_rating_model": null,
    "question_stem": "What is the length of the resulting output in this Python code?\nmy_list =['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\nresult =my_list[2:8:2]\nlen(result)",
    "question_stem_html": null,
    "concept_tags": [
      "Python list slicing",
      "list length"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "10",
    "question_type": "coding",
    "difficulty_rating_manual": 0.32,
    "difficulty_rating_model": null,
    "question_stem": "In the topic of categorical data analysis, we encountered the following plot for visualising how a binary variable varies with a continuous one. We had used the data from heart_failure_clinical_records_dataset.csv Let us try to create the data for such a plot in Python.\n1. Read in the dataset as heart_failure and create the following table, which contains the binned age column, and the proportion of DEATH_EVENT in the second column:\n## age_interval proportion\n## 0 (40, 45] 0.233333\n## 1 (45, 50] 0.324324\n## 2 (50, 55] 0.157895\n## 3 (55, 60] 0.380000\n## 4 (60, 65] 0.208333\n## 5 (65, 70] 0.297297\n## 6 (70, 75] 0.545455\n## 7 (75, 80] 0.500000\n## 8 (80, 85] 0.600000\n## 9 (85, 90] 0.800000\n## 10 (90, 95] 1.000000\n2. Now create this plot using the pandas dataframe, using kind='scatter'.",
    "question_stem_html": null,
    "concept_tags": [
      "categorical data analysis",
      "data visualization",
      "Python pandas"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "11",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.92,
    "difficulty_rating_model": null,
    "question_stem": "Suppose that x is a numpy array with shape (2,2) and y is a numpy array with shape (2,1). What is the name of the numpy function for adding column y as a new column to x (making it have shape (2,3))?",
    "question_stem_html": null,
    "concept_tags": [
      "Numpy arrays",
      "array manipulation",
      "Python numpy"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "12",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.34,
    "difficulty_rating_model": null,
    "question_stem": "In many manufacturing processes, the term work-in-progress is often abbreviated to WIP. In a book manufacturing plant, WIP represents the time it takes for sheets from a press to be folded, gathered, sewn, tipped (with glue) on end sheets, and finally bound together.\nThe data set wip.txt contains samples of 20 books from each of two production plants, and the time for WIP (defined as the time in hours from when the books came off the press till they were packed in cartons).\nThere are two variables in the data set: time and plant (either 1 or 2).\nConsider the following output. For the raw data, when we construct boxplots for each plant, there is a single outlier for each plant (the maximum value in each group).\n## time\n## count mean std min 25% 50% 75% max\n## plant\n## 1 20.0 9.3820 3.997653 4.42 7.4475 8.515 11.045 21.62\n## 2 20.0 11.3535 5.126156 2.33 8.4400 11.960 13.845 25.75\nIf we had applied a log (base e) transform to time before creating the boxplots, would these two points still be outliers? Explain your answer clearly using the summary statistics above only.",
    "question_stem_html": null,
    "concept_tags": [
      "outlier detection",
      "data transformation",
      "boxplots"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "13",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.84,
    "difficulty_rating_model": null,
    "question_stem": "Consider the following two histograms (created with R) from the liverpool dataset used in Tutorial 2:\nWhat is the argument needed to convert the Histogram A into Histogram B?\nhist(liverpool$GF, <argument>)",
    "question_stem_html": null,
    "concept_tags": [
      "data visualization",
      "histograms",
      "R programming"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "14",
    "question_type": "coding",
    "difficulty_rating_manual": 0.64,
    "difficulty_rating_model": null,
    "question_stem": "Using the student-mat.csv dataset from our lectures, create a contingency table from the variables address and guardian. Store it as address_guardian.\nWrite R code that will:\n1. Compute the proportion of students whose home address was rural, and whose guardian was their mother.\n2. Estimate the probability of students whose home address was rural, and whose guardian was their mother, under the null hypothesis of the chi2-test of independence.",
    "question_stem_html": null,
    "concept_tags": [
      "contingency table",
      "R programming",
      "chi-squared test"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "15",
    "question_type": "mcq",
    "difficulty_rating_manual": 1.0,
    "difficulty_rating_model": null,
    "question_stem": "What is the length of the resulting output in this R code?\nvec1 <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j')\nresult <- vec1[2:8][-2]\nlength(result)",
    "question_stem_html": null,
    "concept_tags": [
      "R vector indexing",
      "vector length"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "16",
    "question_type": "coding",
    "difficulty_rating_manual": 0.7,
    "difficulty_rating_model": null,
    "question_stem": "This question is to be answered using R. Suppose that daily demand for newspaper is approximately gamma distributed, with mean 10,000 and variance 1,000,000. At present, the newspaper company prints and distributes C=11,000 copies each day. The profit on each newspaper sold is $1, and the loss on each unsold newspaper is $0.25. Formally, the daily profit function h is h(X)={11000if X≥11000⌊X⌋+(11000−⌊X⌋)(−0.25)if X<11000 where X represents the daily demand. Use simulation to estimate the expected profit per day, for various values of C. Thus recommend the optimal value of C to the company. Ensure that when you make your case, you include confidence intervals, and that you include a visualisation of your results to assist the company in understanding your recommendation. For this question, set your seed to be 2002. You will be awarded more marks for\n* planning your code well,\n* for using functions such as apply instead of for loops.\n* for a clean and clear plot,\n* and for a clear explanation of your results.",
    "question_stem_html": null,
    "concept_tags": [
      "Monte Carlo simulation",
      "gamma distribution",
      "profit optimization"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "17",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.95,
    "difficulty_rating_model": null,
    "question_stem": "Assuming the appropriate package(s) have been installed on the computer, what command needs to be run in order to resolve the following error?",
    "question_stem_html": null,
    "concept_tags": [
      "R packages",
      "library function",
      "error handling"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "18",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.83,
    "difficulty_rating_model": null,
    "question_stem": "After working with R, Python and SAS with one semester, you must have realised some of the strengths/limitations of these software. For each of the three software, list one advantage that you feel it has over the other two. There is no “right” or “wrong” answer, but your response should be a sincere one, and should be backed up with examples from our course material.",
    "question_stem_html": null,
    "concept_tags": [
      "R programming",
      "Python programming",
      "SAS programming"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "19",
    "question_type": "coding",
    "difficulty_rating_manual": null,
    "difficulty_rating_model": null,
    "question_stem": "The Γ(1.5,1) pdf is given by f(y)=1Γ(1.5)y1/2e−y,y>0 where Γ(z)=∫0∞tz−1e−tdt is the gamma function. In this question, we shall use R to generate random variables from this pdf. (You are not allowed to use rgamma() in this question, but you can use rexp() and runif().)\nThe algorithm we shall use is the rejection algorithm. Here is pseudo-code for how it works:\n1. Generate X∼Exp(2/3) and independently, U∼Unif(0,1). To be explicit, the pdf for X is: fX(x)=23exp−2/3x,x>0\n2. If U<(2eX3)1/2e−X/3 then set Y=X and return Y. This is the acceptance step.\n3. Otherwise, return to step 1 and generate a new X and U.\n4. Repeat steps 1 - 3 until a Y is accepted. This Y will follow the Γ(1.5,1) distribution.\nNote that not all X values will be accepted. The acceptance rate is the number of Y acceptednumber of (X,U) pairs generated\n1. Write a function gen_one_Y() that takes no arguments and returns a vector of length two each time it is run: a single Y, and the number of X variables that were needed to get it.\n2. Use this function to generate 105 random variables from f(y).\n3. Create a histogram of the random variables generated, along with the actual pdf.\n4. What is the acceptance rate? In other words, on average how many X variables are generated until a Y is accepted?\n5. The correctness of the above algorithm can be proved theoretically. However, suppose you were given a vector of random variables, and you were told they were from a gamma distribution. Assuming you can use qgamma, explain how you can create a plot to assess if this claim is true. Hint: Think about modifying the qq-plots the we learnt about for normality.",
    "question_stem_html": null,
    "concept_tags": [
      "random variable generation",
      "rejection sampling",
      "gamma distribution"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "20",
    "question_type": "coding",
    "difficulty_rating_manual": null,
    "difficulty_rating_model": null,
    "question_stem": "A sequence is generated using the following recursive relation:\nxn=2xn−1−xn−2,n≥3\nwhere x1=0 and x2=1.\nWrite R code to find x30 and ∑i=130xi.",
    "question_stem_html": null,
    "concept_tags": [
      "recursive sequences",
      "R programming",
      "summation"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "21",
    "question_type": "coding",
    "difficulty_rating_manual": 0.51,
    "difficulty_rating_model": null,
    "question_stem": "This question is to be answered using R. The dataset in taiwan_outlier_removed.csv contains information on housing prices from Taiwan. The dependent variable is price. We had worked with this data in our tutorial; the greatest outlier from there has been removed. In this question, we shall work with price as the dependent variable, and the following two explanatory variables:\n1. ldist: The (natural) log of the distance to the nearest MRT station. The original distance is in meters.\n2. num_stores: The number of convenience stores within walking distance.\nRefer to the attached SAS output, and answer the questions 1–3. Following that, you will have to re-fit the model in R and run the additional analyses.\n1. Interpret the model, when the number of stores is 0. Make sure that your explanation does not involve the logarithm scale.\n2. The adjusted R-squared for this model is 0.59. Your colleague has fitted the same model, but without the log transform of distance. Are the two adjusted R-squared comparable? Why or why not?\n3. Study the SAS plots of residuals versus predictor and assess if there is anything unusual.\n4. Use R to return prediction intervals for the following two cases:\n1. When the number of stores is 5, and the distance to MRT is 2175m.\n2. When the number of stores is 2 and the distance to MRT is 965m.\n5. Identify the most influential point (in absolute value) with regard to the beta coefficient for the number of stores. You can state the ID number of this point.\n6. One of the columns in the influence matrix corresponds to dffit. What is the difference between this column and the unstandardised residuals that we compute?\n7. Re-create the following plot. What do you observe about the relationship between price and ldist, in the presence of num_stores? Hint: what is the association between ldist and num_stores?\nAttachment:\nattachment_for_itemid_258138.pdf",
    "question_stem_html": null,
    "concept_tags": [
      "multiple linear regression",
      "prediction intervals",
      "influential points"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "22",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.92,
    "difficulty_rating_model": null,
    "question_stem": "A clinical trial is conducted to compare the efficacy of drug A and drug B on lowering blood pressure. Participants are randomly divided into two groups. One group is given drug A and the other drug B. The average reduction in blood pressure after taking the drug is measured and compared between the two groups.\nAssuming the distributional assumptions hold, the paired-sample t-test is appropriate in the above scenario (instead of the independent-sample t-test).",
    "question_stem_html": null,
    "concept_tags": [
      "hypothesis testing",
      "t-tests",
      "experimental design"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "23",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.97,
    "difficulty_rating_model": null,
    "question_stem": "Which of the following techniques cannot be used to assess the Normality of a given dataset?",
    "question_stem_html": null,
    "concept_tags": [
      "normality tests",
      "Kolmogorov-Smirnov",
      "Shapiro-Wilk"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "24",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.78,
    "difficulty_rating_model": null,
    "question_stem": "In the One-Way ANOVA, we assume the following model:\nYij=μ+αi+eij,i=1,…,k,j=1,…,ni\nThe use of contr.sum( ) in R corresponds to the following constraint when performing estimation:\n* Setting α1=0.",
    "question_stem_html": null,
    "concept_tags": [
      "ANOVA",
      "contrasts",
      "R programming"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "25",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.91,
    "difficulty_rating_model": null,
    "question_stem": "In the topic on One-Way ANOVA, we worked with the heifers dataset. The following is the SAS output.\nThe value of 0.0148 corresponds to ∑i=1k∑j=1ni(Yij−Y¯¯)2 where Y¯¯ corresponds to the overall mean of all observations.",
    "question_stem_html": null,
    "concept_tags": [
      "ANOVA",
      "sum of squares",
      "SAS output interpretation"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "26",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.77,
    "difficulty_rating_model": null,
    "question_stem": "In the topic on regression, we fitted the following simple linear regression model to the concrete data: Y = β0 + β1X + e\nwhere Y is Flow, and X is amount of Water in the mixture.\nThe summary output for the model (from Python) is as follows:\nThe null hypothesis for the F-test above is\nH0: β1 = β0 = 0",
    "question_stem_html": null,
    "concept_tags": [
      "simple linear regression",
      "F-test",
      "hypothesis testing"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "27",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.78,
    "difficulty_rating_model": null,
    "question_stem": "When assessing robustness of an estimator, one of the properties we consider is the breakdown point.\nFor a particular parameter of interest, an estimator with a large breakdown point is considered to be better than an estimator with a smaller breakdown point.",
    "question_stem_html": null,
    "concept_tags": [
      "robust statistics",
      "breakdown point",
      "estimators"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "28",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.86,
    "difficulty_rating_model": null,
    "question_stem": "Consider the following SAS program:\nDATA ex_1;\nINPUT subject gender $ CA1 CA2 HW $;\nDATALINES;\n10 m 80 84 a ;\n7 m 85 89 a\n;\nWhen the code above was run, there was no output. Which one of the following two steps will fix the error?",
    "question_stem_html": null,
    "concept_tags": [
      "SAS programming",
      "data input",
      "syntax error"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "29",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.64,
    "difficulty_rating_model": null,
    "question_stem": "Simulation can be used to estimate expected values of the form E[g(X)]=∑x=0∞g(x)p(x). The reason we can assume Normality when computing Confidence Intervals is that it is up to us to choose the number of observations to generate.",
    "question_stem_html": null,
    "concept_tags": [
      "Monte Carlo simulation",
      "confidence intervals",
      "central limit theorem"
    ],
    "question_media": [],
    "last_used": null
  },
  {
    "question_id": null,
    "version_id": 1,
    "file_id": null,
    "question_no": "30",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.96,
    "difficulty_rating_model": null,
    "question_stem": "Suppose that X∼N(μ=0,σ2=4). Which of the following R commands will return P(X>2) ?",
    "question_stem_html": null,
    "concept_tags": [
      "normal distribution",
      "R probability functions",
      "pnorm"
    ],
    "question_media": [],
    "last_used": null
  }
]