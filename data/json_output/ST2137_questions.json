[
  {
    "question_no": "1",
    "question_type": "coding",
    "difficulty_rating_manual": 0.41,
    "question_stem": "This question is to be answered using Python.\nConsider the stud_perf dataset once again. There were three measures\nof performance of the students:\nFeature Description Details\nG1 First period grade from 0 to 20\nG2 Second period grade from 0 to 20\nG3 Final grade from 0 to 20\nIn this question, we shall investigate the agreement between G1 and\nG3 scores. If there is a high agreement, it means that teachers can\ndrop one of the scores and still get a good estimate of final grade of a\nstudent.\n1. First, read in the dataset, and divide the G1 and G3 scores into 10\nbins:\n• [0,2], (2,4], (4,6], (6,8], (8,10], (10,12], (12,14], (14,16],\n(16,18], (18,20]\n• You should now have two new columns G1_bin and G3_bin in the\ndataset.\n2. Next, create a contingency table with G1_bin in the rows and\nG3_bin in the columns.\n3. Convert the cell counts into proportions. The sum of entries in all\ncells should now equal to 1. Create a visualisation of this table of\nproportions that is relevant to the goal of measuring agreement.\n4. If we let pij be the proportion in row i and column j, then the\nstrictest version of agreement is η0=∑i=110pii What is the range of\nvalues for η0? Which values correspond to higher agreement?\n5. A less stringent measure of agreement is given by η1=∑|i−j|≤1pij\nCompute η1 separately for the five groups defined by Medu and\nsummarise what you observe.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      1,
      2,
      3
    ],
    "concept_tags": [
      "data analysis",
      "python programming",
      "statistical agreement"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page1.png",
      "data/question_media/ST2137_questions_page3.png",
      "data/question_media/ST2137_questions_page2.png"
    ]
  },
  {
    "question_no": "2",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.23,
    "question_stem": "In 1935, Sir R A Fisher described an experiment involving a British\nwoman. The woman claimed that if she was presented with a cup of\nmilk tea, she would be able to distinguish whether milk or tea was\nadded to the cup first. To test, she was given 8 cups of tea, in four of\nwhich milk was added first.\nThe data collected was as follows:\nActual Milk Actual Tea\nGuessed Milk 3 1\nGuessed Tea 1 3\nSuppose that Fisher’s Exact Test was applied to assess if there was any\nassociation between her guesses and the truth. Under the null\nhypothesis of Fisher’s Exact Test, what is the probability of observing\nthe table above?",
    "question_options": [
      {
        "label": "A",
        "text": "0.5"
      },
      {
        "label": "B",
        "text": "0.25"
      },
      {
        "label": "C",
        "text": "0.5625"
      },
      {
        "label": "D",
        "text": "0.228"
      }
    ],
    "question_answer": "D. 0.228",
    "page_numbers": [
      4
    ],
    "concept_tags": [
      "hypothesis testing",
      "fisher's exact test",
      "probability"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page4.png"
    ]
  },
  {
    "question_no": "3",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.97,
    "question_stem": "Consider the following two histograms (created with Python) from the\nliverpool dataset used in Tutorial 2: (Image - Refer to question paper)\nWhat is the argument needed to convert the Histogram A into\nHistogram B?\nliverpool.GF.hist( 1 )\n1. Choice of: freq=False | density=True | type=\"percent\" | type=\"density\"",
    "question_options": [],
    "question_answer": "freq=False",
    "page_numbers": [
      5
    ],
    "concept_tags": [
      "data visualization",
      "histograms",
      "python programming"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page5.png"
    ]
  },
  {
    "question_no": "4",
    "question_type": "others",
    "difficulty_rating_manual": 0.44,
    "question_stem": "The data in phones.csv contains information on phone calls made in Belgium from 1950 until 1973. Let Y be the number of calls, and X be the year variable. Read the data into Python as a pandas dataframe and answer the following questions:\n1. Write a function that takes in three arguments: beta0, beta1, and\nthe phones dataframe. It should compute the following L1-norm and\nreturn it: ∑i=1n|Yi−β0−β1Xi|\n2. Iterate over a range of beta0 and beta1 values and find the pair\nthat minimizes the L1-norm. Return this pair of beta0 and beta1 values.\n3. Create a plot of this line, along with the OLS estimate, along with\nthe datapoints.\n4. Discuss the benefits of the L1-fit over the OLS fit.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      6
    ],
    "concept_tags": [
      "data analysis",
      "L1-norm",
      "OLS",
      "Python programming"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page6.png"
    ]
  },
  {
    "question_no": "5",
    "question_type": "coding",
    "difficulty_rating_manual": 0.71,
    "question_stem": "Suppose that the data in liverpool_2223_season.csv has been read into\nPython as liverpool. The following code tabulates the goal counts for\nand stores them in a column in goal_counts.\ngoal_counts =pd.DataFrame(np.zeros((10, 2), dtype='int'),\ncolumns=['GF', 'GA'])\ntmp2 =liverpool.GF.value_counts()\ngoal_counts.loc[tmp2.index, 'GF'] =tmp2\nContinue the code to fill up the second column, which tabulates the goal\ncounts against into the second column of goal_counts. Then create the\nfollowing bar chart, which compares the GF and GA:",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      7,
      8
    ],
    "concept_tags": [
      "pandas",
      "data manipulation",
      "data visualization",
      "Python programming"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page7.png",
      "data/question_media/ST2137_questions_page8.png"
    ]
  },
  {
    "question_no": "6",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.99,
    "question_stem": "What is the most likely solution to the error below?",
    "question_options": [
      {
        "label": "A",
        "text": "The function to read the file is read.csv, not read_csv."
      },
      {
        "label": "B",
        "text": "The pandas package has to be imported."
      },
      {
        "label": "C",
        "text": "The pandas package has to be installed."
      },
      {
        "label": "D",
        "text": "The separator has to be specified as sep=';'."
      }
    ],
    "question_answer": "B. The pandas package has to be imported.",
    "page_numbers": [
      9
    ],
    "concept_tags": [
      "Python",
      "pandas",
      "error handling"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page9.png"
    ]
  },
  {
    "question_no": "7",
    "question_type": "open-ended",
    "difficulty_rating_manual": null,
    "question_stem": "The heifers dataset was introduced on in the topic on ANOVA. An\nanalyst used SAS to run the ANOVA procedure and estimate the\ncontrast comparing the Control group to the rest of the five groups. This\nis the code that the analyst used:\nproc glm data=ST2137.HEIFERS;\nclass type;\nmodel org=type / clparm;\nmeans type / hovtest=levene welch plots=none;\nlsmeans type / adjust=tukey pdiff alpha=.05;\nestimate 'control vs. rest' type -1 5 -1 -1 -1 -1 / divisor=5;\nrun;\nThe essential output for this particular contrast can be seen in the\nfollowing figures: (Image - Refer to question paper)",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      10
    ],
    "concept_tags": [
      "ANOVA",
      "SAS",
      "statistical modeling",
      "contrasts"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page10.png"
    ]
  },
  {
    "question_no": "1",
    "question_type": "coding",
    "difficulty_rating_manual": 0.42,
    "question_stem": "Use Python to recreate\n1. the point estimate of the contrast, and\n2. the confidence interval for the contrast.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      11
    ],
    "concept_tags": [
      "Python",
      "statistical inference",
      "confidence intervals"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page11.png"
    ]
  },
  {
    "question_no": "8",
    "question_type": "coding",
    "difficulty_rating_manual": 0.52,
    "question_stem": "This question is to be answered using Python.\nIn Tutorial 6, we considered the notion of a sensitivity curve for assessing the robustness of an estimator. Suppose we have an estimator based on n observations: Tn(x1,x2,…,xn) The sensitivity curve sc(x) when a new observation x is added is given by sc(x)=(n+1)×[Tn+1(x1,…,xn,x)−Tn(x1,…,xn)] Let us consider a newly proposed robust estimator called the broadened median. Given a sample of size n, here is how it is computed:\n• For n odd, the broadened median is\n• the average of the three central order statistics for 5≤n≤12.\n• the average of the five central order statistics for n≥13.\n• For n even, the broadened median is\n• the weighted average of the central four order statistics for\n5≤n≤12, with weights 1/6,1/3,1/3 and 1/6.\n• the weighted average of the central six order statistics for n≥13, with weights 1/10,1/5,1/5,1/5,1/5 and 1/10.\nSuppose we have the following sample of 10 points (n=10):\n2,4,6,7,8,10,14,19,21,28\n1. Plot the sensitivity curve for x∈[5,25] when T represents the broadened median.\n2. Which of the robust estimators that we covered in class is most similar to the broadened median? Explain your answer.\nNote that your code does not have to handle the general case of estimator. It only has to work for the above dataset.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      12,
      13
    ],
    "concept_tags": [
      "Python",
      "robust statistics",
      "estimators",
      "sensitivity curve"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page13.png",
      "data/question_media/ST2137_questions_page12.png"
    ]
  },
  {
    "question_no": "9",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.99,
    "question_stem": "What is the length of the resulting output in this Python code?\nmy_list =['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\nresult =my_list[2:8:2]\nlen(result)",
    "question_options": [
      {
        "label": "A",
        "text": "6"
      },
      {
        "label": "B",
        "text": "4"
      },
      {
        "label": "C",
        "text": "3"
      },
      {
        "label": "D",
        "text": "2"
      }
    ],
    "question_answer": "C. 3",
    "page_numbers": [
      14
    ],
    "concept_tags": [
      "Python",
      "list slicing",
      "data structures"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page14.png"
    ]
  },
  {
    "question_no": "10",
    "question_type": "coding",
    "difficulty_rating_manual": null,
    "question_stem": "In the topic of categorical data analysis, we encountered the following plot for visualising how a binary variable varies with a continuous one. We had used the data from heart_failure_clinical_records_dataset.csv\nLet us try to create the data for such a plot in Python.\n1. Read in the dataset as heart_failure and create the following table, which contains the binned age column, and the proportion of DEATH_EVENT in the second column:\n## age_interval proportion\n## 0 (40, 45] 0.233333\n## 1 (45, 50] 0.324324\n## 2 (50, 55] 0.157895\n## 3 (55, 60] 0.380000\n## 4 (60, 65] 0.208333\n## 5 (65, 70] 0.297297\n## 6 (70, 75] 0.545455\n## 7 (75, 80] 0.500000\n## 8 (80, 85] 0.600000\n## 9 (85, 90] 0.800000\n## 10 (90, 95] 1.000000\n2. Now create this plot using the pandas dataframe, using kind='scatter'.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      15
    ],
    "concept_tags": [
      "Python",
      "data visualization",
      "pandas",
      "categorical data analysis"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page15.png"
    ]
  },
  {
    "question_no": "11",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.92,
    "question_stem": "Suppose that x is a numpy array with shape (2,2) and y is a numpy array with shape (2,1). What is the name of the numpy function for adding column y as a new column to x (making it have shape (2,3))?",
    "question_options": [
      {
        "label": "A",
        "text": "np.cbind()"
      },
      {
        "label": "B",
        "text": "np.stack()"
      },
      {
        "label": "C",
        "text": "np.hstack()"
      },
      {
        "label": "D",
        "text": "np.concatenate()"
      }
    ],
    "question_answer": "C. np.hstack()",
    "page_numbers": [
      17
    ],
    "concept_tags": [
      "numpy",
      "array manipulation",
      "data structures"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page17.png"
    ]
  },
  {
    "question_no": "12",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.34,
    "question_stem": "In many manufacturing processes, the term work-in-progress is often abbreviated to WIP. In a book manufacturing plant, WIP represents the time it takes for sheets from a press to be folded, gathered, sewn, tipped (with glue) on end sheets, and finally bound together. The data set wip.txt contains samples of 20 books from each of two production plants, and the time for WIP (defined as the time in hours from when the books came off the press till they were packed in cartons). There are two variables in the data set: time and plant (either 1 or 2). Consider the following output. For the raw data, when we construct boxplots for each plant, there is a single outlier for each plant (the maximum value in each group).\n## time\n## count mean std min 25% 50% 75% max\n## plant\n## 1 20.0 9.3820 3.997653 4.42 7.4475 8.515 11.045 21.62\n## 2 20.0 11.3535 5.126156 2.33 8.4400 11.960 13.845 25.75\nIf we had applied a log (base e) transform to time before creating the boxplots, would these two points still be outliers? Explain your answer clearly using the summary statistics above only.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      18
    ],
    "concept_tags": [
      "data analysis",
      "outliers",
      "data transformation",
      "summary statistics"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page18.png"
    ]
  },
  {
    "question_no": "13",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.84,
    "question_stem": "Consider the following two histograms (created with R) from the liverpool dataset used in Tutorial 2: (Image - Refer to question paper) What is the argument needed to convert the Histogram A into Histogram B? hist(liverpool$GF, 1 )",
    "question_options": [
      {
        "label": "A",
        "text": "freq=FALSE"
      },
      {
        "label": "B",
        "text": "density=TRUE"
      },
      {
        "label": "C",
        "text": "type=\"density\""
      },
      {
        "label": "D",
        "text": "type=\"percent\""
      }
    ],
    "question_answer": "B. density=TRUE",
    "page_numbers": [
      19
    ],
    "concept_tags": [
      "R programming",
      "data visualization",
      "histograms"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page19.png"
    ]
  },
  {
    "question_no": "14",
    "question_type": "coding",
    "difficulty_rating_manual": 0.64,
    "question_stem": "Using the student-mat.csv dataset from our lectures, create a contingency table from the variables address and guardian. Store it as address_guardian. Write R code that will:\n1. Compute the proportion of students whose home address was rural, and whose guardian was their mother.\n2. Estimate the probability of students whose home address was rural, and whose guardian was their mother, under the null hypothesis of the chi2-test of independence.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      20
    ],
    "concept_tags": [
      "R programming",
      "data manipulation",
      "contingency tables",
      "probability",
      "chi-squared test"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page20.png"
    ]
  },
  {
    "question_no": "15",
    "question_type": "mcq",
    "difficulty_rating_manual": 1.0,
    "question_stem": "What is the length of the resulting output in this R code?\nvec1 <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j')\nresult <- vec1[2:8][-2]\nlength(result)",
    "question_options": [
      {
        "label": "A",
        "text": "3"
      },
      {
        "label": "B",
        "text": "2"
      },
      {
        "label": "C",
        "text": "4"
      },
      {
        "label": "D",
        "text": "6"
      }
    ],
    "question_answer": "D. 6",
    "page_numbers": [
      "21"
    ],
    "concept_tags": [
      "R programming",
      "vector manipulation",
      "data structures"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "16",
    "question_type": "coding",
    "difficulty_rating_manual": 0.7,
    "question_stem": "This question is to be answered using R.\nSuppose that daily demand for newspaper is approximately gamma\ndistributed, with mean 10,000 and variance 1,000,000. At present, the\nnewspaper company prints and distributes C=11,000 copies each day.\nThe profit on each newspaper sold is $1, and the loss on each unsold\nnewspaper is $0.25. Formally, the daily profit function h is\nh(X)={11000if X≥11000⌊X⌋+(11000−⌊X⌋)(−0.25)if X<11000\nwhere X represents the daily demand. Use simulation to estimate the\nexpected profit per day, for various values of C. Thus recommend the\noptimal value of C to the company.\nEnsure that when you make your case, you include confidence intervals,\nand that you include a visualisation of your results to assist the\ncompany in understanding your recommendation. For this question, set\nyour seed to be 2002.\nYou will be awarded more marks for\n• planning your code well,\n• for using functions such as apply instead of for loops.\n• for a clean and clear plot,\n• and for a clear explanation of your results.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      "22"
    ],
    "concept_tags": [
      "R programming",
      "simulation",
      "gamma distribution",
      "optimization"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "17",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.95,
    "question_stem": "Assuming the appropriate package(s) have been installed on the\ncomputer, what command needs to be run in order to resolve the\nfollowing error?",
    "question_options": [
      {
        "label": "A",
        "text": "install.packages(\"psych\")"
      },
      {
        "label": "B",
        "text": "load(psych)"
      },
      {
        "label": "C",
        "text": "library(psych)"
      },
      {
        "label": "D",
        "text": "library(lattice)"
      }
    ],
    "question_answer": "C. library(psych)",
    "page_numbers": [
      "23"
    ],
    "concept_tags": [
      "R programming",
      "package management"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "18",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.83,
    "question_stem": "After working with R, Python and SAS with one semester, you must\nhave realised some of the strengths/limitations of these software. For\neach of the three software, list one advantage that you feel it has over\nthe other two. There is no “right” or “wrong” answer, but your response\nshould be a sincere one, and should be backed up with examples from\nour course material.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      "24"
    ],
    "concept_tags": [
      "programming languages",
      "R",
      "Python",
      "SAS"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "19",
    "question_type": "coding",
    "difficulty_rating_manual": null,
    "question_stem": "The Γ(1.5,1) pdf is given by\nf(y)=1Γ(1.5)y1/2e−y,y>0 where Γ(z)=∫0∞tz−1e−tdt is the gamma\nfunction. In this question, we shall use R to generate random variables\nfrom this pdf. (You are not allowed to use rgamma() in this question,\nbut you can use rexp() and runif().)\nThe algorithm we shall use is the rejection algorithm. Here is\npseudo-code for how it works:\n1. Generate X∼Exp(2/3) and independently, U∼Unif(0,1). To be\nexplicit, the pdf for X is: fX(x)=23exp−2/3x,x>0\n2. If U<(2eX3)1/2e−X/3 then set Y=X and return Y. This is the\nacceptance step.\n3. Otherwise, return to step 1 and generate a new X and U.\n4. Repeat steps 1 - 3 until a Y is accepted. This Y will follow the\nΓ(1.5,1) distribution.\nNote that not all X values will be accepted. The acceptance rate is the\nnumber of Y acceptednumber of (X,U) pairs generated\n1. Write a function gen_one_Y() that takes no arguments and\nreturns a vector of length two each time it is run: a single Y, and the\nnumber of X variables that were needed to get it.\n2. Use this function to generate 105 random variables from f(y).\n3. Create a histogram of the random variables generated, along with\nthe actual pdf.\n4. What is the acceptance rate? In other words, on average how\nmany X variables are generated until a Y is accepted?\n5. The correctness of the above algorithm can be proved\ntheoretically. However, suppose you were given a vector of random\nvariables, and you were told they were from a gamma distribution.\nAssuming you can use qgamma, explain how you can create a plot to\nassess if this claim is true. Hint: Think about modifying the qq-plots the\nwe learnt about for normality.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      "25"
    ],
    "concept_tags": [
      "R programming",
      "random variable generation",
      "rejection sampling",
      "gamma distribution",
      "QQ plot"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "20",
    "question_type": "coding",
    "difficulty_rating_manual": null,
    "question_stem": "A sequence is generated using the following recursive relation:\nxn=2xn−1−xn−2,n≥3\nwhere x1=0 and x2=1.\nWrite R code to find x30 and ∑i=130xi.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      27
    ],
    "concept_tags": [
      "R programming",
      "sequences",
      "recursion"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page27.png"
    ]
  },
  {
    "question_no": "21.1",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.51,
    "question_stem": "Interpret the model, when the number of stores is 0. Make sure that your explanation does not involve the logarithm scale.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      28,
      29
    ],
    "concept_tags": [
      "regression",
      "model interpretation"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page28.png",
      "data/question_media/ST2137_questions_page29.png"
    ]
  },
  {
    "question_no": "21.2",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.51,
    "question_stem": "The adjusted\nfor this model is 0.59. Your colleague has fitted the same model, but\nwithout the log transform of distance. Are the two adjusted\ncomparable? Why or why not?",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      28,
      29
    ],
    "concept_tags": [
      "regression",
      "R-squared",
      "model comparison"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page28.png",
      "data/question_media/ST2137_questions_page29.png"
    ]
  },
  {
    "question_no": "21.3",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.51,
    "question_stem": "Study the SAS plots of residuals versus predictor and assess if\nthere is anything unusual.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      28,
      29
    ],
    "concept_tags": [
      "regression",
      "residual analysis",
      "model diagnostics"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page28.png",
      "data/question_media/ST2137_questions_page29.png"
    ]
  },
  {
    "question_no": "21.4",
    "question_type": "coding",
    "difficulty_rating_manual": 0.51,
    "question_stem": "Use R to return prediction intervals for the following two cases:\n1. When the number of stores is 5, and the distance to MRT is\n2175m.\n2. When the number of stores is 2 and the distance to MRT is 965m.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      28,
      29
    ],
    "concept_tags": [
      "R programming",
      "prediction intervals",
      "regression"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page28.png",
      "data/question_media/ST2137_questions_page29.png"
    ]
  },
  {
    "question_no": "21.5",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.51,
    "question_stem": "Identify the most influential point (in absolute value) with regard\nto the beta coefficient for the number of stores. You can state the ID\nnumber of this point.",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      28,
      29
    ],
    "concept_tags": [
      "regression",
      "influential points",
      "model diagnostics"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page28.png",
      "data/question_media/ST2137_questions_page29.png"
    ]
  },
  {
    "question_no": "21.6",
    "question_type": "open-ended",
    "difficulty_rating_manual": 0.51,
    "question_stem": "One of the columns in the influence matrix corresponds to dffit.\nWhat is the difference between this column and the unstandardised\nresiduals that we compute?",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      29
    ],
    "concept_tags": [
      "regression",
      "influence measures",
      "residuals"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page29.png"
    ]
  },
  {
    "question_no": "21.7",
    "question_type": "coding",
    "difficulty_rating_manual": 0.51,
    "question_stem": "Re-create the following plot. What do you observe about the\nrelationship between price and ldist, in the presence of num_stores?\nHint: what is the association between ldist and num_stores?",
    "question_options": [],
    "question_answer": null,
    "page_numbers": [
      29
    ],
    "concept_tags": [
      "R programming",
      "data visualization",
      "regression"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page29.png"
    ]
  },
  {
    "question_no": "22",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.92,
    "question_stem": "A clinical trial is conducted to compare the efficacy of drug A and drug B\non lowering blood pressure. Participants are randomly divided into two\ngroups. One group is given drug A and the other drug B. The average\nreduction in blood pressure after taking the drug is measured and\ncompared between the two groups.\nAssuming the distributional assumptions hold, the paired-sample t-test\nis appropriate in the above scenario (instead of the independent-sample\nt-test).",
    "question_options": [
      {
        "label": "A",
        "text": "True"
      },
      {
        "label": "B",
        "text": "False"
      }
    ],
    "question_answer": "B. False",
    "page_numbers": [
      30
    ],
    "concept_tags": [
      "hypothesis testing",
      "t-test",
      "experimental design"
    ],
    "page_image_paths": [
      "data/question_media/ST2137_questions_page30.png"
    ]
  },
  {
    "question_no": "23",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.97,
    "question_stem": "Which of the following techniques cannot be used to assess the\nNormality of a given dataset?",
    "question_options": [
      {
        "label": "A",
        "text": "Kolmogorov-Smirnov test"
      },
      {
        "label": "B",
        "text": "Shapiro-Wilk test"
      },
      {
        "label": "C",
        "text": "Skewness"
      },
      {
        "label": "D",
        "text": "1-sample t-test"
      }
    ],
    "question_answer": "D. 1-sample t-test",
    "page_numbers": [
      "31"
    ],
    "concept_tags": [
      "normality testing",
      "statistical tests",
      "hypothesis testing"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "24",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.78,
    "question_stem": "In the One-Way ANOVA, we assume the following model:\nYij=μ+αi+eij,i=1,…,k,j=1,…,ni\nThe use of contr.sum( ) in R corresponds to the following constraint\nwhen performing estimation:\n• Setting α1=0.",
    "question_options": [
      {
        "label": "A",
        "text": "True"
      },
      {
        "label": "B",
        "text": "False"
      }
    ],
    "question_answer": "B. False",
    "page_numbers": [
      "32"
    ],
    "concept_tags": [
      "ANOVA",
      "linear models",
      "R programming"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "25",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.91,
    "question_stem": "In the topic on One-Way ANOVA, we worked with the heifers dataset.\nThe following is the SAS output.\nThe value of 0.0148 corresponds to ∑i=1k∑j=1ni(Yij−Y¯¯)2 where Y¯¯\ncorresponds to the overall mean of all observations.",
    "question_options": [
      {
        "label": "A",
        "text": "True"
      },
      {
        "label": "B",
        "text": "False"
      }
    ],
    "question_answer": "B. False",
    "page_numbers": [
      "33"
    ],
    "concept_tags": [
      "ANOVA",
      "sum of squares",
      "SAS output"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "26",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.77,
    "question_stem": "In the topic on regression, we fitted the following simple linear\nregression model to the concrete data:\nwhere\n•\n: is the Flow, and\n•\n: is amount of Water in the mixture.\nThe summary output for the model (from Python) is as follows:\nThe null hypothesis for the F-test above is\nH0:β1=β0=0",
    "question_options": [
      {
        "label": "A",
        "text": "True"
      },
      {
        "label": "B",
        "text": "False"
      }
    ],
    "question_answer": "B. False",
    "page_numbers": [
      "34",
      "35"
    ],
    "concept_tags": [
      "regression",
      "hypothesis testing",
      "F-test"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "27",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.78,
    "question_stem": "When assessing robustness of an estimator, one of the properties we consider is the breakdown point. For a particular parameter of interest, an estimator with a large breakdown point is considered to be better than an estimator with a smaller breakdown point.",
    "question_options": [
      {
        "label": "A",
        "text": "True"
      },
      {
        "label": "B",
        "text": "False"
      }
    ],
    "question_answer": "A. True",
    "page_numbers": [
      "36"
    ],
    "concept_tags": [
      "robustness",
      "estimators",
      "breakdown point"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "28",
    "question_type": "coding",
    "difficulty_rating_manual": 0.86,
    "question_stem": "Consider the following SAS program:\nDATA ex_1;\nINPUT subject gender $ CA1 CA2 HW $;\nDATALINES;\n10 m 80 84 a ;\n7 m 85 89 a\n;\nWhen the code above was run, there was no output. Which one of the following two steps will fix the error?",
    "question_options": [
      {
        "label": "A",
        "text": "Removing the semi-colon on line 4."
      },
      {
        "label": "B",
        "text": "Moving the semi-colon on line 6 to the end of line 5."
      }
    ],
    "question_answer": "A. Removing the semi-colon on line 4.",
    "page_numbers": [
      "37"
    ],
    "concept_tags": [
      "SAS programming",
      "debugging",
      "data step"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "29",
    "question_type": "mcq",
    "difficulty_rating_manual": 0.64,
    "question_stem": "Simulation can be used to estimate expected values of the form E[g(X)]=∑x=0∞g(x)p(x). The reason we can assume Normality when computing Confidence Intervals is that it is up to us to choose the number of observations to generate.",
    "question_options": [
      {
        "label": "A",
        "text": "True"
      },
      {
        "label": "B",
        "text": "False"
      }
    ],
    "question_answer": "A. True",
    "page_numbers": [
      "38"
    ],
    "concept_tags": [
      "simulation",
      "expected values",
      "confidence intervals",
      "normality"
    ],
    "page_image_paths": []
  },
  {
    "question_no": "30",
    "question_type": "coding",
    "difficulty_rating_manual": 0.96,
    "question_stem": "Suppose that X∼N(μ=0,σ2=4). Which of the following R commands will return P(X>2) ?",
    "question_options": [
      {
        "label": "A",
        "text": "pnorm(2, mean = 0, sd = 2, lower.tail = FALSE)"
      },
      {
        "label": "B",
        "text": "pnorm(2, mean = 0, var = 4, lower.tail = FALSE)"
      },
      {
        "label": "C",
        "text": "1 - qnorm(2, mean = 0, sd = 2)"
      },
      {
        "label": "D",
        "text": "1 - qnorm(2, mean = 0, var = 4)"
      }
    ],
    "question_answer": "A. pnorm(2, mean = 0, sd = 2, lower.tail = FALSE)",
    "page_numbers": [
      "39"
    ],
    "concept_tags": [
      "R programming",
      "normal distribution",
      "probability",
      "pnorm"
    ],
    "page_image_paths": []
  }
]